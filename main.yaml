---
- hosts: etcd
  remote_user: root
  gather_facts: true
  vars:
    SERVICE_CLUSTER_IP_RANGE: "10.0.0.0/24"
    KUBE_APISERVER_PORT: "7443"
    ETCD_PEER_NODES: "{% for h in groups['etcd'] %}{{ hostvars[h]['NODE_NAME'] }}=https://{{ h }}:2380,{% endfor %}"
    ETCD_PEER_CLUSTER: "{{ ETCD_PEER_NODES.rstrip(',') }}"
    ETCD_CLIENT_NODES: "{% for h in groups['etcd'] %}https://{{ h }}:2379,{% endfor %}"
    ETCD_CLIENT_CLUSTER: "{{ ETCD_CLIENT_NODES.rstrip(',') }}"
    HOSTS: "{% for h in groups['etcd'] %}'\"{{ h }}\"',{% endfor %}"
    CER_HOSTS: "{{ HOSTS.rstrip(',') }}"
  tasks:
  - name: Bootstrap a host without python2 installed
    raw: apt install -y python2.7
  - name: Prerequisites:/etc/hosts
    blockinfile:
      path: /etc/hosts
      block: |
             192.168.81.197 master k8s-master-01
             192.168.81.198 master k8s-master-02
             192.168.81.199 node1 k8s-worker-01
             192.168.81.200 node2 k8s-worker-02
  - name: Prerequisites:resolve.conf
    lineinfile:
      path: /etc/resolv.conf
      regexp: '^nameserver 114.114.114.114'
      insertbefore: '^nameserver '
      line: 'nameserver 114.114.114.114'
    tags: insert
  - name: Prerequisites:br_netfilter
    modprobe:
      name: br_netfilter
      state: present
  - name: Prerequisites:sysctl
    sysctl:
      name: "{{ item }}"
      sysctl_file: /etc/sysctl.d/k8s.conf
      value: 1
      sysctl_set: yes
      state: present
      reload: yes
    with_items:
      - net.ipv4.ip_forward
      - net.bridge.bridge-nf-call-ip6tables
      - net.bridge.bridge-nf-call-iptables
  - name: Resolving prerequisites
    script: requirement.sh 
  - name: Set hostname
    hostname:
      name: "{{ HOST }}"
  - name: Install the package "conntrack"
    apt:
      name: "{{ item }}" 
      state: present
    with_items:
      - conntrack
      - rsync
    tags: apt 
  - name: Create some directory
    file:
      path: "{{ item }}"
      state: directory
      mode: 0755
    #with_items: [ /opt/etcd/bin, /opt/etcd/cfg, /opt/etcd/ssl, /root/TLS/etcd, /root/TLS/k8s, /opt/docker, /opt/kubernetes/bin, /opt/kubernetes/cfg, /opt/kubernetes/ssl, /opt/kubernetes/logs ]
    loop: 
      - /opt/etcd/bin
      - /opt/etcd/cfg
      - /opt/etcd/ssl
      - /root/TLS/etcd 
      - /root/TLS/k8s 
      - /opt/docker 
      - /opt/kubernetes/bin 
      - /opt/kubernetes/cfg 
      - /opt/kubernetes/ssl
      - /opt/kubernetes/logs 
      - /opt/haproxy/bin
      - /opt/haproxy/cfg 
      - /opt/haproxy/logs 
      - /opt/keepalived/bin
      - /opt/keepalived/cfg 
      - /opt/keepalived/logs 

  - name: Download all required bin files
    copy: 
      src: '{{ item.src }}'
      dest: '{{ item.dest }}'
      mode: 0755
    with_items:
      - { src: /root/binary-kubernetes-double-master/cfssl/, dest: /usr/local/bin }
      - { src: /root/binary-kubernetes-double-master/etcd/, dest: /opt/etcd }
      - { src: /root/binary-kubernetes-double-master/docker-20.10.7/, dest: /opt/docker }
      - { src: /root/binary-kubernetes-double-master/kubernetes-v1.20.8/, dest: /opt/kubernetes }
  #haproxy && keepalived
  - block:
      - include_tasks: /root/binary-kubernetes-double-master/haproxy.yaml

  #keepalived
      - include_tasks: /root/binary-kubernetes-double-master/keepalived.yaml
    when: ROLE == "master"

  #etcd
  - name: etcd:Generate the etcd certificate
    script: /root/binary-kubernetes-double-master/certificate-etcd.sh {{ CER_HOSTS }}
    when: NODE_NAME == "etcd1"
    #  - name: etcd:Register names of etcd certificate as ansible vairable
    #shell: (cd /root/TLS/etcd; find -maxdepth 1 -type f) | cut -d'/' -f2
    #register: files_to_copy
    #when: NODE_NAME == "etcd1"
  - name: etcd:Fetch etcd certificates to the ansible loaclhost
    fetch:
      src: "{{ item }}"
      dest: /root/binary-kubernetes-double-master/certificate-etcd/ 
      flat: yes
      validate_checksum: no
    with_fileglob: 
      - /root/TLS/etcd/*
    when: NODE_NAME == "etcd1"
  - name: etcd:Distribute certificates to all etcd cluster nodes
    copy:
      src: /root/binary-kubernetes-double-master/certificate-etcd/
      dest: /opt/etcd/ssl

  - name: etcd:Deploy etcd cluster
    script: /root/binary-kubernetes-double-master/etcd.sh
  - name: etcd:Generate etcd configration file
    template: src=/root/binary-kubernetes-double-master/etcd.conf.j2 dest=/opt/etcd/cfg/etcd.conf 
  - name: etcd:Manager etcd service by systemd
    systemd:
      name: etcd
      daemon_reload: yes
      enabled: yes
      state: started

  - name: etcd:Check etcd cluster status
    shell: ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="{{ETCD_CLIENT_CLUSTER}}" endpoint health
    register: result
  - debug: 
      var: result.stderr_lines
      verbosity: 0
  #docker
  - name: docker:Deploy container engine--docker
    script: /root/binary-kubernetes-double-master/docker.sh
  - name: docker:Check container engine docker status
    shell: docker version | egrep -A 1 'Client|Engine'
    register: docker_check_result
  - debug:
      var: docker_check_result.stdout_lines
      verbosity: 0

  #kube-apiserver
  - name: kube-apiserver:Generate the kube-apiserver certificate
    script: /root/binary-kubernetes-double-master/certificate-kube-apiserver.sh {{ CER_HOSTS }} {{ MASTER_VIP }}
    when: ROLE == "master" and HOST == "k8s-master-01" and HOST == "k8s-master-01"
  - include_tasks: /root/binary-kubernetes-double-master/synchronize.yaml
  - block:
      - name: kube-apiserver:Generate kube-apiserver configration file
        template: src=/root/binary-kubernetes-double-master/kube-apiserver.conf.j2 dest=/opt/kubernetes/cfg/kube-apiserver.conf
      - name: kube-apiserver:Deploy the kube-apiserver
        script: /root/binary-kubernetes-double-master/kube-apiserver.sh
      - name: kube-apiserver:Check kube-apiserver status
        shell: /opt/kubernetes/bin/kube-apiserver --version
        register: kubeapiserver_check_result
      - debug:
          var: kubeapiserver_check_result.stdout_lines
          verbosity: 0
    rescue:
      - fail: msg="kube-apiserver deploy failed"
    always:
      - debug: msg="kube-apiserver deploy sucessfully"
    when: ROLE == "master" 

  #kube-controller-manager
  - name: kube-controller-manager:Generate the kube-controller-manager certificate
    script: /root/binary-kubernetes-double-master/certificate-controller-manager.sh
    when: ROLE == "master" and HOST == "k8s-master-01" and HOST == "k8s-master-01"
  - include_tasks: /root/binary-kubernetes-double-master/synchronize.yaml
  - block:
      - name: kube-controller-manager:Deploy the kube-controller-manager 
        script: /root/binary-kubernetes-double-master/kube-controller-manager.sh https://{{inventory_hostname}}:6443 {{CONTROLLER_SCHEDULER}}
      - name: kube-controller-manager:Check kube-controller-manager status
        shell: /opt/kubernetes/bin/kube-controller-manager --version
        register: kubecontrollermanager_check_result
      - debug:
          var: kubecontrollermanager_check_result.stdout_lines
          verbosity: 0
    rescue:
      - fail: msg="kube-controller-manager deploy failed"
    always:
      - debug: msg="kube-controller-manager deploy sucessfully"
    when: ROLE == "master" and HOST == "k8s-master-01" and HOST == "k8s-master-01"

  #kube-scheduler
  - name: kube-scheduler:Generate the kube-scheduler certificate
    script: /root/binary-kubernetes-double-master/certificate-scheduler.sh
    when: ROLE == "master" and HOST == "k8s-master-01"
  - include_tasks: /root/binary-kubernetes-double-master/synchronize.yaml
  - block:
      - name: kube-scheduler:Deploy the kube-scheduler
        script: /root/binary-kubernetes-double-master/kube-scheduler.sh https://{{MASTER_VIP}}:{{KUBE_APISERVER_PORT}} {{CONTROLLER_SCHEDULER}}
      - name: kube-scheduler:Check kube-scheduler status
        shell: /opt/kubernetes/bin/kube-scheduler --version
        register: kubescheduler_check_result
      - debug:
          var: kubescheduler_check_result.stdout_lines
          verbosity: 0
    rescue:
      - fail: msg="kube-scheduler deploy failed"
    always:
      - debug: msg="kube-scheduler deploy sucessfully"
    when: ROLE == "master" and HOST == "k8s-master-01"


  #kube-admin
  - name: kubelet:Decompression kubernetes compression package on worker nodes
    command: tar zxvf kubernetes-server-linux-amd64.tar.gz 
    args:
      chdir: /opt/kubernetes/
      creates: /opt/kubernetes/kubernetes
  - name: kube-admin:Generate the kube-admin certificate
    script: /root/binary-kubernetes-double-master/certificate-admin.sh
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kube-admin:Register names of kube-admin certificate as ansible vairable
    shell: (cd /root/TLS/k8s; find -maxdepth 1 -name '*admin*' -type f ) | cut -d'/' -f2
    register: files_to_copy
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kube-admin:Fetch kube-admin certificates to the ansible localhost
    fetch:
      src: /root/TLS/k8s/{{ item }}
      dest: /root/binary-kubernetes-double-master/certificate-k8s/ 
      flat: yes
      validate_checksum: no
    with_items: 
      - "{{ files_to_copy.stdout_lines }}"
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kube-admin:Distribute kube-admin certificates to master and node nodes
    copy:
      src: /root/binary-kubernetes-double-master/certificate-k8s/
      dest: /opt/kubernetes/ssl
  - name: kube-admin:Gernerate kube-admin kubeconfig
    script: /root/binary-kubernetes-double-master/kube-admin.sh https://{{MASTER_VIP}}:{{KUBE_APISERVER_PORT}}
  - block:
      - wait_for: host=127.0.0.1 port=10251
      - wait_for: host=127.0.0.1 port=10252
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kube-admin:Check kube-admin status
    shell: /usr/bin/kubectl get cs
    register: kube_admin_check_result
  - debug:
      var: kube_admin_check_result.stdout_lines
      verbosity: 0
    when: ROLE == "master" and HOST == "k8s-master-01"

  #kubelet
  - name: kubelet:Authorizes kubete-bootstrap user request the certificate
    shell: kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap 
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kubelet:Fetch kube-apiserver token.cvs from master node
    fetch:
      src: /opt/kubernetes/cfg/token.csv
      dest: /root/binary-kubernetes-double-master/token/ 
      flat: yes
      validate_checksum: no
    when: ROLE == "master" and HOST == "k8s-master-01"
    #  - name: kubelet:Distribute token value to worker nodes
    #copy:
    #  src: /root/binary-kubernetes-double-master/token/token.csv
    #  dest: /tmp
    #- name: kubelet:Declare and register an variable for the value of token
    #shell: cat  /tmp/token.csv  | awk -F',' '{print $1}' 
    #register: TOKEN
  - name: kubelet:Deploy kubelet on master and worker node
    script: /root/binary-kubernetes-double-master/kubelet.sh https://{{MASTER_VIP}}:{{KUBE_APISERVER_PORT}} {{ lookup('file','/root/binary-kubernetes-double-master/token/token.csv') | regex_search("^[0-9a-zA-Z]*") }}
    #- name: kubelet:Delete token.csv file in tmp directory 
    #file:
    #  path: /tmp/token.csv
    #  state: absent   
  - systemd:
      name: kubelet
      state: restarted
  - name: Wait_for kubelet bootstrap
    script: /root/binary-kubernetes-double-master/check-node.sh
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kubelet:Approve certificate request
    shell: kubectl certificate approve $(kubectl get csr | grep "Pending" | awk '{print $1}')
    when: ROLE == "master" and HOST == "k8s-master-01"
  - pause: seconds=10
  - name: kubelet:Check kubelet status
    shell: kubectl get csr,nodes
    register: kubelet_check_result
    when: ROLE == "master" and HOST == "k8s-master-01"
  - debug:
      var: kubelet_check_result.stdout_lines
      verbosity: 0
    when: ROLE == "master" and HOST == "k8s-master-01"


  #kube-proxy
  - name: kube-proxy:Generate the kube-proxy certificate
    script: /root/binary-kubernetes-double-master/certificate-kube-proxy.sh
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kube-proxy:Register names of kube-proxy certificate as ansible vairable
    shell: (cd /root/TLS/k8s; find -maxdepth 1 -name '*kube-proxy*' -type f ) | cut -d'/' -f2
    register: files_to_copy
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kube-proxy:Fetch kube-proxy certificates to the ansible localhost
    fetch:
      src: /root/TLS/k8s/{{ item }}
      dest: /root/binary-kubernetes-double-master/certificate-k8s/ 
      flat: yes
      validate_checksum: no
    with_items: 
      - "{{ files_to_copy.stdout_lines }}"
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: kube-proxy:Distribute kube-proxy certificates to master and worker nodes
    copy:
      src: /root/binary-kubernetes-double-master/certificate-k8s/
      dest: /opt/kubernetes/ssl
  - name: kube-proxy:Deploy kube-proxy
    script: /root/binary-kubernetes-double-master/kube-proxy.sh https://{{MASTER_VIP}}:{{KUBE_APISERVER_PORT}}
  - name: kube-proxy:Check kube-proxy status
    shell: systemctl status  kube-proxy.service
    register: kube_proxy_check_result
    tags: proxy
  - debug:
      var: kube_proxy_check_result.stdout_lines
      verbosity: 0
    tags: proxy

  #Calico
  - name: Calico:Download calico YAML file
    get_url:
      url: '{{ item.url }}'
      dest: '{{ item.dest }}'
      mode: 0755
    with_items:
      - { url: 'https://docs.projectcalico.org/manifests/calico.yaml', dest: /opt/kubernetes/cfg/calico.yaml }
      - { url: 'https://github.com/projectcalico/calicoctl/releases/download/v3.19.1/calicoctl', dest: /usr/bin/calicoctl }
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: Calico:Deploy calico
    shell: "{{ item }}"
    loop:
      - kubectl apply -f /opt/kubernetes/cfg/calico.yaml
      - kubectl apply -f /opt/kubernetes/cfg/apiserver-to-kubelet-rbac.yaml
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: Calico:make configuration for calicoctl
    script: /root/binary-kubernetes-double-master/calicoctl.sh {{ ETCD_CLIENT_CLUSTER }}
    tags: calicoctl
    when: ROLE == "master" and HOST == "k8s-master-01"
  - name: Calico:wait for calico port started
    wait_for: port=9099 state=started
  - name: Calico:Check calico status
    shell: calicoctl node status; calicoctl get nodes
    register: calico_check_result
    when: ROLE == "master" and HOST == "k8s-master-01"
  - debug:
      var: calico_check_result.stdout_lines
      verbosity: 0
    when: ROLE == "master" and HOST == "k8s-master-01"

  #coredns
  - name: Deploy coredns for kubernetes service
    script: /root/binary-kubernetes-double-master/coredns.sh 
    when: ROLE == "master" and HOST == "k8s-master-01"
